# Feature Engineering
For project #7, Artificial Intelligence SP25

This lecture on feature engineering would fit just before the lectures on machine learning start, based on our current schedules. I think it would be useful for students to learn a bit about steps they should take before exploring all the possible machine learning algorithms and neural networks. ML models in particular rely a lot on the quality of the input data. I think that learning about the features' impact on the ML algorithms will also help students understand how the different algorithms work. Finally, I believe that clustering and dimensionality reduction are highly relevant, and are useful skills/topics to know.

This repo includes a set of slides that would be used during lecture. Slides which ask for discussion expect ~5 minutes to be taken for students to exchange ideas. Resources are provided at the end for students to explore notebooks in their own time for practice. 

In particular, these notebooks would be recommended for exploration: 
*https://www.kaggle.com/learn/feature-engineering
*https://www.kaggle.com/code/alexisbcook/scaling-and-normalization

For homework, students could be asked to work through a DataCamp course and submit a certificate of completion. For example: https://www.datacamp.com/courses/feature-engineering-for-machine-learning-in-python. Alternatively or additionally, this video may be interesting: https://www.youtube.com/watch?v=ft77eXtn30Q. 

Link to slides: https://docs.google.com/presentation/d/1ERNsORdKpY1AZNTCGknsjvNT22zJBhalEumCcbwCHtU/edit?usp=sharing
